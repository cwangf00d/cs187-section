{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cindy Section 1: Optimization using PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwangf00d/cs187-section/blob/master/Cindy_Section_1_Optimization_using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivVJhT2SlZB_"
      },
      "source": [
        "# CS 187 Section 1: Optimization using PyTorch\n",
        "\n",
        "URL: https://bit.ly/section1pytorch\n",
        "\n",
        "Partly adapted from [this tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
        "\n",
        "In our labs, you have seen numerous examples of tensor operations in PyTorch. However, PyTorch is most widely known as a toolkit for optimizing neural networks. In this section, we will learn how to use PyTorch to optimize an objective function. We will start from low level PyTorch tensor operations, gradually wrapping low-level operations using high-level classes/functions to simplify the code. By the end of this section, you should be able to understand the concepts of parameters, gradients, and the optimization process. This will be the foudation for implementing neural networks with much more parameters in future labs.\n",
        "\n",
        "Goals:\n",
        "\n",
        "1. Understand what is computation graph and how automatic differentiation works.\n",
        "2. Implement automatic differentiation in PyTorch.\n",
        "3. Implement optimization using high-level PyTorch abstractions `nn.Module` and `optim`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch9M8ANzvSIY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLxf_gdkELfw"
      },
      "source": [
        "# Set up plotting\n",
        "plt.style.use('tableau-colorblind10')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwEkuStbfoqs"
      },
      "source": [
        "## Gradient-Based Optimization\n",
        "\n",
        "Consider the following problem:\n",
        "\n",
        "$$\n",
        "\\min_x {(x-2)}^2.\n",
        "$$\n",
        "\n",
        "Let's pretend that we don't know how to solve this problem analytically. Instead, we will use this problem to demonstrate gradient-based optimization. We will define ${(x-2)}^2$ as our _loss function_: $L(x) = {(x-2)}^2$. We can visualize this loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmBdJ0Nv6Aq"
      },
      "source": [
        "def loss(x):\n",
        "  return (x - 2) ** 2\n",
        "\n",
        "x_vals = torch.linspace(-6, 6, 100)\n",
        "y_vals = loss(x_vals)\n",
        "\n",
        "plt.plot(x_vals, y_vals)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPyU-XYYv8gi"
      },
      "source": [
        "In gradient-based optimization, we first initialize $x$ to be a random value, and then we perform the following steps:\n",
        "\n",
        "1. Find the gradient of the loss function, the direction in which it is increasing fastest.\n",
        "2. Take a step in the opposite direction.\n",
        "3. Repeat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlLisPCoawj"
      },
      "source": [
        "## Manual Differentiation\n",
        "\n",
        "For this particular problem, we can manually calculate the gradient of the loss function:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L(x)}{\\partial x} & = \\frac{\\partial L(x)}{\\partial (x-2)} \\cdot \\frac{\\partial (x-2)}{\\partial x}\\\\\n",
        "&= 2(x-2) \\cdot 1\\\\\n",
        "& = 2(x-2)\n",
        "\\end{align}\n",
        "\n",
        "Based on this calculation, we can implement a gradient-based optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN89bxUUxm-M"
      },
      "source": [
        "def manual_grad(x):\n",
        "  return 2 * (x - 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jfkOf3qPB-"
      },
      "source": [
        "First, we initialize $x$ to be, say, -4. We visualize both the initial value of $x$ and the curve of the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Vwxw3zqVE1"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "\n",
        "plt.plot(x_vals, y_vals, '-', label='$(x-2)^2$')\n",
        "plt.plot(x, loss(x), 'o', label='x')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmqDa4Y0xv0W"
      },
      "source": [
        "Next, we take a step in the opposite direction. We will update $x$ as\n",
        "\n",
        "$$\n",
        "x \\leftarrow x - \\eta \\frac{\\partial loss(x)}{\\partial x},\n",
        "$$\n",
        "where $\\eta$ is the learning rate. We'll use a learning rate of 0.1 for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx_tMemAydSx"
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O4o8ejPvZb5"
      },
      "source": [
        "plt.plot(x_vals, y_vals, '-', label='$(x-2)^2$')\n",
        "plt.plot(x, loss(x), 'o', label='old x')\n",
        "\n",
        "update = - learning_rate * manual_grad(x)\n",
        "x_new = x + update\n",
        "\n",
        "plt.arrow(x, loss(x), update, 0, head_width=1, head_length=0.2, length_includes_head=True)\n",
        "plt.axvline(x_new, ls='--', color='k')\n",
        "\n",
        "x = x_new\n",
        "\n",
        "plt.plot(x, loss(x), 'o', label='new x')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvGPt7eA1n4i"
      },
      "source": [
        "Let's repeat this process multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ7TSYvz1wYq"
      },
      "source": [
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "all_xs = []\n",
        "all_ys = []\n",
        "all_updates = []\n",
        "\n",
        "num_iters = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "def loss(x):\n",
        "  return (x - 2) ** 2\n",
        "\n",
        "# Initialize x\n",
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "\n",
        "for iter in range(num_iters):\n",
        "  all_xs.append(x.item())\n",
        "  L = loss(x)\n",
        "  all_ys.append(L.item())\n",
        "  print (f'iter: {iter}, loss: {L}, x: {x}')\n",
        "  all_updates.append((- learning_rate * manual_grad(x)).item())\n",
        "  x.data += - learning_rate * manual_grad(x)\n",
        "all_xs.append(x.item())\n",
        "all_ys.append((loss(x)).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qSNnto0Qurg"
      },
      "source": [
        "We can visualize this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YvEAG6FQyNF"
      },
      "source": [
        "def animate(all_xs, all_updates):\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_tight_layout(True)\n",
        "\n",
        "  line, = ax.plot(x_vals, y_vals, 'b-', label='$(x-2)^2$')\n",
        "  xold, = ax.plot(all_xs[0], all_ys[0], 'o', label='old x')\n",
        "\n",
        "  vline = ax.axvline(all_xs[1], ls='--', color='k')\n",
        "\n",
        "  xnew, = ax.plot(all_xs[1], all_ys[1], 'o', label='new x')\n",
        "  ax.legend()\n",
        "  global patch\n",
        "  patch = None\n",
        "  def update(i):\n",
        "    global patch\n",
        "    if patch in ax.patches:\n",
        "      ax.patches.remove(patch) \n",
        "    num_updates = i//2\n",
        "    if i % 2 == 0:\n",
        "      label = f'update {num_updates} gradient computation'\n",
        "      xold.set_visible(True)\n",
        "      xnew.set_visible(False)\n",
        "      vline.set_visible(False)\n",
        "      \n",
        "      patch = plt.Arrow(all_xs[num_updates], all_ys[num_updates], all_updates[num_updates], 0, width=2, color='k')\n",
        "      ax.add_patch(patch)\n",
        "    else:\n",
        "      label = f'update {num_updates} finished'\n",
        "\n",
        "      xold.set_visible(False)\n",
        "      xnew.set_visible(True)\n",
        "      vline.set_visible(True)\n",
        "\n",
        "      vline.set_xdata([all_xs[num_updates+1], all_xs[num_updates+1]])\n",
        "    \n",
        "      xnew.set_xdata([all_xs[num_updates+1],])\n",
        "      xnew.set_ydata([all_ys[num_updates+1],])\n",
        "\n",
        "      xold.set_xdata([all_xs[num_updates+1],])\n",
        "      xold.set_ydata([all_ys[num_updates+1],])\n",
        "    ax.set_xlabel(label)\n",
        "    return line, ax\n",
        "\n",
        "  anim = FuncAnimation(fig, update, frames=range(num_iters*2), interval=1500)\n",
        "  return anim\n",
        "\n",
        "animate(all_xs, all_updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV3T93NbDz8f"
      },
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "While we can manually derive the gradient for a simple loss function with a single parameter, it would be tedious, if at all possible, to derive the gradient for a complex loss function with millions of parameters. As indicated by its name, *automatic differentiation* alleviates us from such burdens by automatically computing the gradients of a loss function with respect to its inputs.\n",
        "\n",
        "Let's jump to an example of how to compute gradients in PyTorch. In order to enable automatic differentiation, we first need to wrap a tensor in a `torch.nn.Parameter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9A3dPjqF1ds"
      },
      "source": [
        "x_init = -4.              # initial parameter value\n",
        "x = torch.tensor(x_init)  # tensorize it\n",
        "x = nn.Parameter(x)       # declare it to be a parameter to optimize\n",
        "\n",
        "## We build the loss function from its one parameter\n",
        "z = x - 2\n",
        "z.retain_grad()           # make sure to compute gradient here\n",
        "L = z ** 2                # the loss \n",
        "\n",
        "## and compute the gradients of the loss `L`\n",
        "L.backward()\n",
        "\n",
        "## Now, we can examine the gradients\n",
        "print (f'Automatic differentiation result: {x.grad}, manual differentiation result: {manual_grad(x)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JOe3BXhFCdm"
      },
      "source": [
        "Voila! How does it work? To understand how automatic differentiation works, we need to first understand the concept of a computation graph.\n",
        "\n",
        "### Computation Graph\n",
        "\n",
        "Let's take a look at what underlying data structure PyTorch uses to store a tensor. We will use a helper function to visualize the underlying data structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho-To3TBFltC"
      },
      "source": [
        "!wget -q  https://raw.githubusercontent.com/nlp-course/data/master/scripts/makedot.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNzN8elz_EXR"
      },
      "source": [
        "from makedot import make_dot\n",
        "make_dot(L, params={'x': x, 'z': z, 'L':L})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWUbmzhxSreO"
      },
      "source": [
        "Even for this basic computation, PyTorch creates an internal computation graph. Starting from $x$, it first applies a subtraction function `Sub` to compute $x-2$, and then applies a power function `Pow` with $x-2$ as input to compute ${(x-2)}^2$, which is the output $L$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMyUowlcTrLn"
      },
      "source": [
        "**Student Question:** Looking at this computation graph, why can we compute $\\frac{\\partial L}{\\partial x}$ by simplying calling `L.backward()` (without mentioning `x` at all)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb6SdSzqUCk_"
      },
      "source": [
        "As another demonstration of the computation graph, we can find the parent and the grandparent of the output node `l`. Note that `z` is an intermediate result instead of a variable, so there's no `parent.variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTRMV0jYUT41"
      },
      "source": [
        "print (f\"Number of parents of l: {len(L.grad_fn.next_functions)}\")\n",
        "\n",
        "parent = L.grad_fn.next_functions[0][0]\n",
        "\n",
        "print (f\"id of parent: {id(parent)}, id of z's grad_fn: {id(z.grad_fn)}\")\n",
        "\n",
        "grandparent = parent.next_functions[0][0]\n",
        "\n",
        "print (f\"id of grandparent: {id(grandparent.variable)}, id of x: {id(x)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUjJzwAmY1VN"
      },
      "source": [
        "Now that we can understand why it is possible to compute $\\frac{\\partial L}{\\partial x}$ by simplying calling `L.backward()`, the next question is how does automatic differentiation work. The answer is the *chain rule* for differentiation, as illustrated below.\n",
        "\n",
        "When we call `L.backward()`, it will first compute the gradients with respect to `L`'s parents:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp1_new.png\" width=150 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAo5N4jNhuzm"
      },
      "source": [
        "print (z.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8xX_BDNhqmK"
      },
      "source": [
        "Then grandparents using the chain rule:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp2_new.png\" width=150 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtAHodNuYQr4"
      },
      "source": [
        "In summary, PyTorch creates a computation graph during the forward pass, and during the backward pass, it backpropagates gradients from the output node all the way back to the input nodes.\n",
        "\n",
        "Now, let's try a more complex computation graph with multiple input nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njHnHkBDYcZ6"
      },
      "source": [
        "a = torch.tensor(3.)\n",
        "b = torch.tensor(4.)\n",
        "c = torch.tensor(5.)\n",
        "\n",
        "a = nn.Parameter(a)\n",
        "b = nn.Parameter(b)\n",
        "c = nn.Parameter(c)\n",
        "\n",
        "L = ((a + b) * c) + c\n",
        "make_dot(L, params={'a': a, 'b': b, 'c': c, 'L': L})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-gLhP92iKrs"
      },
      "source": [
        "Similarly, we can compute $\\frac{\\partial L}{\\partial a}$, $\\frac{\\partial L}{\\partial b}$, and $\\frac{\\partial L}{\\partial c}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMU8mtyiivG3"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp21_new.png\" width=300 />\n",
        "\n",
        "When we call `L.backward()`, it will first backpropagate gradients to the parent nodes of `L`:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp22_new.png\" width=300 />\n",
        "\n",
        "then grandparents:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp23_new.png\" width=300 />\n",
        "\n",
        "and then great grandparents:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/section1_bp24_new.png\" width=300 />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leZHN3POiY7L"
      },
      "source": [
        "L.backward()\n",
        "print (f\"dL/da: {a.grad}\")\n",
        "print (f\"dL/db: {b.grad}\")\n",
        "print (f\"dL/dc: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlu9h44A5N7r"
      },
      "source": [
        "### Gradient Accumulation\n",
        "\n",
        "Note that a node in the computation graph might have multiple children (in English, that variable was used multiple times in the function), such as `c` in the above example. In such cases, we need to accumulate gradients coming from different children. Therefore, it's the default behavior of PyTorch, and unless we explicitly set gradients of a tensor to be zero, they will accumulate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQl-0uP55Vb7"
      },
      "source": [
        "a = torch.tensor(3.)\n",
        "b = torch.tensor(4.)\n",
        "c = torch.tensor(5.)\n",
        "\n",
        "a = nn.Parameter(a)\n",
        "b = nn.Parameter(b)\n",
        "c = nn.Parameter(c)\n",
        "\n",
        "L = ((a + b) * c) + c\n",
        "L.backward()\n",
        "\n",
        "print (f\"first backward  dL/da: {a.grad}\")\n",
        "print (f\"first backward  dL/db: {b.grad}\")\n",
        "print (f\"first backward  dL/dc: {c.grad}\\n\")\n",
        "\n",
        "L = ((a + b) * c) + c\n",
        "L.backward()\n",
        "print (f\"second backward dL/da: {a.grad}\")\n",
        "print (f\"second backward dL/db: {b.grad}\")\n",
        "print (f\"second backward dL/dc: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBKnTlvP5tfQ"
      },
      "source": [
        "To set the gradient to zero, we use `a.grad.fill_(0)` since `a.grad` is also a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAQenTEI53lD"
      },
      "source": [
        "a.grad.fill_(0)\n",
        "b.grad.fill_(0)\n",
        "c.grad.fill_(0)\n",
        "L = ((a + b) * c) + c\n",
        "L.backward()\n",
        "\n",
        "print (f\"first backward  dL/da: {a.grad}\")\n",
        "print (f\"first backward  dL/db: {b.grad}\")\n",
        "print (f\"first backward  dL/dc: {c.grad}\\n\")\n",
        "\n",
        "a.grad.fill_(0)\n",
        "b.grad.fill_(0)\n",
        "c.grad.fill_(0)\n",
        "L = ((a + b) * c) + c\n",
        "L.backward()\n",
        "print (f\"second backward dL/da: {a.grad}\")\n",
        "print (f\"second backward dL/db: {b.grad}\")\n",
        "print (f\"second backward dL/dc: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEeMJJMHTUW"
      },
      "source": [
        "### `torch.no_grad`\n",
        "\n",
        "You can disable gradient computation using context `torch.no_grad`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVMQ8areHbws"
      },
      "source": [
        "x_init = -4.              # initial parameter value\n",
        "x = torch.tensor(x_init)  # tensorize it\n",
        "x = nn.Parameter(x)       # declare it to be a parameter to optimize\n",
        "\n",
        "## We build the loss function from its one parameter\n",
        "with torch.no_grad():\n",
        "  z = x - 2\n",
        "  L = z ** 2                # the loss \n",
        "\n",
        "## and compute the gradients of the loss `L`\n",
        "L.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNQ0wyFvHy8I"
      },
      "source": [
        "Why did it fail? What's the new computation graph?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a6HRXSPH0_3"
      },
      "source": [
        "make_dot(L, params={'x': x, 'z': z, 'L':L})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eax9wN1LH5YY"
      },
      "source": [
        "**Student Question:** In what scenarios would we want to disable gradient computation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJc3AShqoUa9"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "Now we can perform the optimization process using PyTorch's automatic differentiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIzJjbb9pn5z"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "x = nn.Parameter(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhNHKypTphO6"
      },
      "source": [
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "all_xs = []\n",
        "all_ys = []\n",
        "all_updates = []\n",
        "\n",
        "num_iters = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "def loss(x):\n",
        "  return (x - 2) ** 2\n",
        "\n",
        "# Initialize x\n",
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "\n",
        "for iter in range(num_iters):\n",
        "  x = nn.Parameter(x)\n",
        "  all_xs.append(x.item())\n",
        "  L = loss(x)\n",
        "  all_ys.append(L.item())\n",
        "  print (f'iter: {iter}, loss: {L}, x: {x}')\n",
        "  L.backward()\n",
        "  all_updates.append((- learning_rate * x.grad).item())\n",
        "  x.data += - learning_rate * x.grad\n",
        "  x.grad.fill_(0)\n",
        "all_xs.append(x.item())\n",
        "all_ys.append((loss(x)).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jX_dR8SRGBS"
      },
      "source": [
        "animate(all_xs, all_updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntjCDtY5r5gM"
      },
      "source": [
        "## Scaling Up: PyTorch `nn.Module` and `optim`\n",
        "\n",
        "We have seen how to use PyTorch automatic differentiation to optimize a few parameters. However, what if there are millions of parameters? Do we need to write a few million updates in the form of $x = x - \\eta \\frac{\\partial loss}{\\partial x}$? PyTorch provides high-level abstractions to make it much easier to scale up the number of parameters (and the complexity of the function).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud8aQ6vpBliQ"
      },
      "source": [
        "Before we start, let's recap optimization using low-level PyTorch functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5i0qKhyB4dM"
      },
      "source": [
        "num_iters = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "def loss(x):\n",
        "  return (x - 2) ** 2\n",
        "\n",
        "# Initialize x\n",
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "\n",
        "for iter in range(num_iters):\n",
        "  x = nn.Parameter(x)\n",
        "  L = loss(x)\n",
        "  print (f'iter: {iter}, loss: {L}, x: {x}')\n",
        "  L.backward()\n",
        "  x.data += - learning_rate * x.grad\n",
        "  x.grad.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOukrunBBhpk"
      },
      "source": [
        "### `nn.Module` class\n",
        "\n",
        "To use PyTorch's high-level abstractions, we first need to wrap the definition of the forward pass in subclass of a `nn.Module` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ-UzOcMsAal"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  # initializer, defines parameters\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    x_init = -4.\n",
        "    x = torch.tensor(x_init)\n",
        "    self.x = nn.Parameter(x)\n",
        "\n",
        "  # defines the forward computation\n",
        "  def forward(self):\n",
        "    return (self.x - 2) ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2aAjHSywEkx"
      },
      "source": [
        "We can instantiate a `Model` object. By calling `.parameters()`, we will see a list of parameters contained within it. By calling its `forward` function, we can compute the loss as well. Note that the `forward` function does not take any input here, but in real problems it usually does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO8ENW5vwHVb"
      },
      "source": [
        "model = Model()\n",
        "parameters = list(model.parameters())\n",
        "print (f'The parameters of model: {parameters}')\n",
        "L = model() # calling model() is the same as calling model.forward()\n",
        "print (f'Loss value: {L}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmncZeuS2PPq"
      },
      "source": [
        "`nn.Module` also provides a `named_parameters` function, which returns the names of each parameter as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRSGMiZl2Y_m"
      },
      "source": [
        "print (list(model.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcTBChExwJvG"
      },
      "source": [
        "We can rewrite the optimization process using this new class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svbthB8BwKu8"
      },
      "source": [
        "num_iters = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize x\n",
        "model = Model()\n",
        "\n",
        "for iter in range(num_iters):\n",
        "  L = model()\n",
        "  print (f'iter: {iter}, loss: {L}, x: {model.x}')\n",
        "  L.backward()\n",
        "  model.x.data += - learning_rate * model.x.grad\n",
        "  model.x.grad.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oka9AnM-wPV2"
      },
      "source": [
        "### `optim` class\n",
        "\n",
        "PyTorch defines a class for optimizers, which is responsible for updating the parameters of the model. For example, the below two code blocks are equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbc8KGQ7w2cI"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "\n",
        "x = nn.Parameter(x)\n",
        "L = loss(x)\n",
        "\n",
        "L.backward()\n",
        "x.data += - learning_rate * x.grad\n",
        "x.grad.fill_(0)\n",
        "print (f'new x: {x}, new x gradient: {x.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skW3kAknwSaB"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "x = nn.Parameter(x)\n",
        "\n",
        "parameters = [x,]\n",
        "optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n",
        "  \n",
        "L = loss(x)\n",
        "  \n",
        "L.backward()\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()\n",
        "print (f'new x: {x}, new x gradient: {x.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uAft3ExxRA8"
      },
      "source": [
        "Note that SGD stands for Stochastic Gradient Descent. We usually use SGD optimizer with a random batch from the entire training set, hence the name \"stochastic\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnH2jqfixm68"
      },
      "source": [
        "While the above two code blocks are of roughtly the same size (the optimizer one is actually longer), when we have millions of parameters, the second code block would relieve us from the burden of writing millions of parameter update equations. For example, let's look at a case with three parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxIfVfgYyTSc"
      },
      "source": [
        "def loss_3params(x, y, z):\n",
        "  return (x - 2) ** 2 + (y - x) ** 2 + (z - y) ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ZCzAvSyRms"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "x = nn.Parameter(x)\n",
        "\n",
        "y_init = -3.\n",
        "y = torch.tensor(y_init)\n",
        "y = nn.Parameter(y)\n",
        "\n",
        "z_init = -2.\n",
        "z = torch.tensor(z_init)\n",
        "z = nn.Parameter(z)\n",
        "\n",
        "L = loss_3params(x, y, z)\n",
        "\n",
        "L.backward()\n",
        "\n",
        "x.data += - learning_rate * x.grad\n",
        "x.grad.fill_(0)\n",
        "y.data += - learning_rate * y.grad\n",
        "y.grad.fill_(0)\n",
        "z.data += - learning_rate * z.grad\n",
        "z.grad.fill_(0)\n",
        "print (f'new x: {x}, new y: {y}, new z: {z}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8ladjaxMkg"
      },
      "source": [
        "x_init = -4.\n",
        "x = torch.tensor(x_init)\n",
        "x = nn.Parameter(x)\n",
        "\n",
        "y_init = -3.\n",
        "y = torch.tensor(y_init)\n",
        "y = nn.Parameter(y)\n",
        "\n",
        "z_init = -2.\n",
        "z = torch.tensor(z_init)\n",
        "z = nn.Parameter(z)\n",
        "\n",
        "parameters = [x, y, z]\n",
        "optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n",
        "  \n",
        "L = loss_3params(x, y, z)\n",
        "  \n",
        "L.backward()\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()\n",
        "print (f'new x: {x}, new y: {y}, new z: {z}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY3Xkh5IznfU"
      },
      "source": [
        "### Putting Everything Together\n",
        "\n",
        "Now we can put everything together and use the PyTorchy way to create and optimize a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmc8VVfVzBGs"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  # initializer, defines parameters\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    val_init = -4.\n",
        "    val_tensor = torch.tensor(val_init)\n",
        "\n",
        "    self.x = nn.Parameter(val_tensor)\n",
        "    self.y = nn.Parameter(val_tensor)\n",
        "    self.z = nn.Parameter(val_tensor)\n",
        "\n",
        "  # defines the forward computation\n",
        "  def forward(self):\n",
        "    return (self.x - 2) ** 2 + (y - x) ** 2 + (z - y) ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwfmTSDzVd3"
      },
      "source": [
        "num_iters = 20\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize x\n",
        "model = Model()\n",
        "parameters = model.parameters()\n",
        "optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n",
        "\n",
        "for iter in range(num_iters):\n",
        "  L = model()\n",
        "  print (f'iter: {iter}, loss: {L}, x: {model.x}, y: {model.y}, z: {model.z}')\n",
        "  L.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Faf4tF1W00PT"
      },
      "source": [
        "## Predefined Modules in PyTorch\n",
        "\n",
        "PyTorch has predefined some `nn.Module` classes for us, such as a linear function (in deep learning terms, a linear layer), or a sigmoid function (a nonlinear activation function).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCzszeIf0m8q"
      },
      "source": [
        "linear_layer = nn.Linear(in_features=3, out_features=1)\n",
        "\n",
        "print (list(linear_layer.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAm1SRII13pT"
      },
      "source": [
        "Check out PyTorch code here: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EudHBGiE1wOq"
      },
      "source": [
        "in_features = torch.tensor([1., 2., 3.])\n",
        "print (linear_layer(in_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8tb7OBZ2uhl"
      },
      "source": [
        "We can demonstrate its equivalence to using low-level tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5NOhfF_24rl"
      },
      "source": [
        "weight_tensor = linear_layer.weight\n",
        "bias_tensor = linear_layer.bias\n",
        "\n",
        "output = (weight_tensor * in_features).sum() + bias_tensor\n",
        "print (output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzBjSd1w2rJS"
      },
      "source": [
        "Let's also try a nonlinear sigmoid layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnAc_6C52jUA"
      },
      "source": [
        "sigmoid_layer = nn.Sigmoid()\n",
        "print (sigmoid_layer(in_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUxm-c6Y3Fz5"
      },
      "source": [
        "A very useful property of a `nn.Module` class is that it will collect all parameters of its members in its own parameters, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TopJnPs3Wig"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  # initializer, defines parameters\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.linear_1 = nn.Linear(3, 4)\n",
        "    self.linear_2 = nn.Linear(4, 4)\n",
        "    self.linear_3 = nn.Linear(4, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  # defines the forward computation\n",
        "  def forward(self, x):\n",
        "    return self.linear_3(self.linear_2(self.sigmoid(self.linear_1(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKo7Jk2O3qXP"
      },
      "source": [
        "model = Model()\n",
        "print (list(model.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ookKlEx43tg4"
      },
      "source": [
        "in_features = torch.tensor([1., 2., 3.])\n",
        "output = model(in_features)\n",
        "print (output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fify8e8S4Pyj"
      },
      "source": [
        "Let's look at its computation graph. Remember that all these PyTorch modules are just wrappers for the basic tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvEXluEv3yFH"
      },
      "source": [
        "make_dot(output, params=dict(model.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6hzCfzhMTUi"
      },
      "source": [
        "**Student Question:** Can you implement a gradient-descent based linear regression algorithm in PyTorch now? What should be the loss function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2sVUBvmIBQS"
      },
      "source": [
        "# End of Section 1"
      ]
    }
  ]
}